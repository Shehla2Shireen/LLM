# -*- coding: utf-8 -*-
"""LLM_Class_Activity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mn29JBL3mN5FnZsKDWt-z9rmCI247Ygz
"""

# !pip install transformers datasets peft accelerate bitsandbytes trl
# !pip install -U transformers datasets peft accelerate bitsandbytes trl reportlab

import transformers
print(transformers.__version__)  # should be >= 4.54.0

from transformers import TrainingArguments
print(TrainingArguments.__init__.__code__.co_varnames)

!pip install evaluate

import nltk
import evaluate
import numpy as np
from datasets import load_dataset
from transformers import T5Tokenizer, DataCollatorForSeq2Seq
from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Load the dataset from Hugging Face
dataset = load_dataset("tatsu-lab/alpaca")
'''
data = {
    "question": [
        "What is artificial intelligence?",
        "Who wrote the book '1984'?",
        "What is the capital of France?"
    ],
    "answer": [
        "Artificial intelligence is the simulation of human intelligence in machines.",
        "George Orwell wrote the book '1984'.",
        "The capital of France is Paris."
    ]
}
'''
print(dataset)
print(dataset["train"][0])  # Show first example
'''
DatasetDict({
    train: Dataset({
        features: ['instruction', 'input', 'output', 'text'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['instruction', 'input', 'output', 'text'],
        num_rows: 10570
    })
})

'''



# Cell 1
from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq

MODEL_NAME = "google/flan-t5-base"
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
model_name = MODEL_NAME  # For later use (PDF report)

dataset

# Cell 2
from datasets import load_dataset

# Load Alpaca dataset
dataset = load_dataset("yahma/alpaca-cleaned")

# Create train/test split if not already present
if "test" not in dataset:
    dataset = dataset["train"].train_test_split(test_size=0.3)

dataset

# Cell 3
prefix = "Please answer this question: "

def preprocess_function(examples):
    """Add prefix, tokenize input and output."""
    if "input" in examples:
        inputs = [
            prefix + instr + (" " + inp if inp else "")
            for instr, inp in zip(examples["instruction"], examples["input"])
        ]
    else:
        inputs = [prefix + instr for instr in examples["instruction"]]

    model_inputs = tokenizer(inputs, max_length=128, truncation=True)

    labels = tokenizer(
        text_target=examples["output"],
        max_length=512,
        truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

print(dataset.column_names)

# Cell 4
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Cell 5
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

!pip install reportlab
!pip install evaluate rouge_score

# !pip install -U transformers

import numpy as np
import evaluate

rouge_metric = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=-1) if predictions.ndim > 1 else predictions
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    return {key: value * 100 for key, value in result.items()}

import transformers
print(transformers.__version__)

# Cell 6 (Optimized for T4 GPU)
!pip install -q transformers datasets peft accelerate bitsandbytes trl reportlab

import os
import torch
import importlib
import transformers
importlib.reload(transformers)
from transformers import Trainer, TrainingArguments, set_seed
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
import numpy as np
import evaluate

set_seed(42)

# Enable CUDA if available
if torch.cuda.is_available():
    model = model.to("cuda")
    torch.backends.cuda.matmul.allow_tf32 = True
    model.config.use_cache = False  # Disable cache for gradient checkpointing

# Ensure LoRA layers require gradients
for param in model.parameters():
    param.requires_grad = True


# Enable gradient checkpointing to save memory
model.gradient_checkpointing_enable()

# ------------------------
# Metrics (ROUGE)
# ------------------------
rouge_metric = evaluate.load("rouge")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    return {key: round(value * 100, 2) for key, value in result.items()}

# ------------------------
# Training arguments
# ------------------------
output_dir = "./flan_t5_lora_alpaca"
batch_size = 2           # Reduced for T4 GPU
num_epochs = 3

args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=8,  # Higher accumulation to simulate bigger batch
    learning_rate=1e-4,
    num_train_epochs=num_epochs,
    logging_strategy="steps",
    logging_steps=50,
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    fp16=True,            # Enable mixed precision
    report_to="none"
)

# ------------------------
# Trainer
# ------------------------
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset["train"].shuffle(seed=42).select(range(2000)),
    eval_dataset=tokenized_dataset["test"].select(range(min(200, len(tokenized_dataset["test"])))),
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Cell 7
# Resume training if a checkpoint is
# Cell 7
from transformers.trainer_utils import get_last_checkpoint

last_checkpoint = get_last_checkpoint(output_dir)
if last_checkpoint is not None:
    print(f"Resuming from checkpoint: {last_checkpoint}")
    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)
else:
    print("No checkpoint found. Starting training from scratch.")
    train_result = trainer.train()

trainer.save_model(output_dir)

# Cell 8
def generate(model, tokenizer, instruction, input_text=None, max_new_tokens=128):
    if input_text and len(input_text.strip()) > 0:
        prompt = f"{prefix}{instruction}\n\nInput: {input_text}"
    else:
        prompt = f"{prefix}{instruction}"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            num_beams=4
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

test_instructions = [
    ("Give three tips for staying healthy.", None),
    ("Explain overfitting in simple terms.", None),
    ("Translate to French:", "How are you today?"),
    ("Summarize:", "Large language models are trained on vast amounts of data to predict the next token."),
    ("Write a Python function to compute Fibonacci numbers.", None),
]

model.eval()
samples = []
for instr, inp in test_instructions:
    out = generate(model, tokenizer, instr, inp)
    samples.append((instr, inp, out))
    print("Instruction:", instr)
    if inp:
        print("Input:", inp)
    print("Model output:", out)
    print("=" * 80)

# Cell 9
pdf_path = os.path.join(output_dir, "report.pdf")

def create_pdf(
    pdf_path,
    model_name,
    dataset_name,
    lora_cfg: LoraConfig,
    epochs,
    train_samples,
    eval_samples,
    sample_generations
):
    c = canvas.Canvas(pdf_path, pagesize=A4)
    width, height = A4
    margin = 40
    y = height - margin

    def write_line(text, size=10, lead=14):
        nonlocal y
        if y < margin + 50:
            c.showPage()
            y = height - margin
        c.setFont("Helvetica", size)
        c.drawString(margin, y, text)
        y -= lead

    write_line("Instruction Tuning Report", size=16, lead=20)
    write_line(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    write_line("")
    write_line("=== Model & Dataset ===", size=12, lead=16)
    write_line(f"Base model: {model_name}")
    write_line(f"Dataset: {dataset_name}")
    write_line("")
    write_line("=== LoRA Config ===", size=12, lead=16)
    write_line(f"r={lora_cfg.r}, alpha={lora_cfg.lora_alpha}, dropout={lora_cfg.lora_dropout}, bias={lora_cfg.bias}")
    write_line(f"task_type={lora_cfg.task_type}")
    write_line("")
    write_line("=== Training ===", size=12, lead=16)
    write_line(f"Epochs: {epochs}")
    write_line(f"Train samples (used): {train_samples}")
    write_line(f"Eval samples (used): {eval_samples}")
    write_line("")
    write_line("=== Sample Generations ===", size=12, lead=16)

    for i, (instr, inp, out) in enumerate(sample_generations, 1):
        write_line(f"{i}. Instruction: {instr}")
        if inp:
            write_line(f"   Input: {inp}")
        write_line(f"   Output: {out[:500]}")
        write_line("")

    c.save()

create_pdf(
    pdf_path=pdf_path,
    model_name=model_name,
    dataset_name="yahma/alpaca-cleaned",
    lora_cfg=lora_config,
    epochs=num_epochs,
    train_samples=2000,
    eval_samples=min(200, len(tokenized_dataset["test"])),
    sample_generations=samples
)

print(f"\nPDF report saved to: {pdf_path}")







